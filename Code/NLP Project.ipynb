{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg, conll2000\n",
    "from nltk.corpus import wordnet as wn\n",
    "train_data = conll2000.tagged_sents('train.txt') + brown.tagged_sents(categories=['news'])\n",
    "test_data = conll2000.tagged_sents('test.txt')\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),                # gerunds\n",
    "    (r'.*ed$', 'VBD'),                 # simple past\n",
    "    (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),                # modals\n",
    "    (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                  # plural nouns\n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'.*', 'NN')                      # nouns (default)\n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=regexp_tagger)\n",
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunker\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = \"<END>\", \"<END>\"\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "    return {\"pos\": pos,\n",
    "            \"word\": word,\n",
    "            \"prevpos\": prevpos,\n",
    "            \"nextpos\": nextpos, \n",
    "            \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \n",
    "            \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i)}  \n",
    "\n",
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "\n",
    "\n",
    "\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train( \n",
    "            train_set, algorithm=None, trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/anaconda3/lib/python3.7/site-packages/nltk/classify/maxent.py:1386: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/home/emil/anaconda3/lib/python3.7/site-packages/nltk/classify/maxent.py:1388: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/home/emil/anaconda3/lib/python3.7/site-packages/nltk/classify/maxent.py:1389: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt')\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.wsd import lesk\n",
    "import json\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "engine = inflect.engine()\n",
    "\n",
    "def simplify(sent, bigrams):\n",
    "    sents = []\n",
    "    start = 0\n",
    "    for i in range(len(bigrams)):\n",
    "        pair = bigrams[i]\n",
    "        w1, t1 = pair[0]\n",
    "        w2, t2 = pair[1]\n",
    "        if w1 == ',' and t2.startswith('W'):\n",
    "            sents.append(sent[start:i])\n",
    "            start = i + 1\n",
    "            end = start\n",
    "            while end < len(sent) and sent[end] != ',':\n",
    "                end += 1\n",
    "            sents.append(sent[start:end])\n",
    "            if end < len(sent):\n",
    "                sents[-2].extend(sent[end + 1:])\n",
    "            sents[-1][0] = 'He/she/it/they/there'\n",
    "            break\n",
    "    if not sents:\n",
    "        sents.append(sent)\n",
    "    return sents\n",
    "    \n",
    "def refine(sent):\n",
    "    if sent[-1] == ',':\n",
    "        sent[-1] = '.'\n",
    "    if not sent[0].istitle():\n",
    "        sent[0] = sent[0].title()\n",
    "    if sent[-1].isalpha():\n",
    "        sent.append('.')\n",
    "    return sent\n",
    "\n",
    "def decompose(sent, tagger):\n",
    "    sents = []\n",
    "    tagged_sent = tagger.tag(sent)\n",
    "    bigrams = list(nltk.trigrams(tagged_sent))\n",
    "    start = 0\n",
    "    for i in range(start, len(bigrams)):\n",
    "        triple = bigrams[i]\n",
    "        w1, t1 = triple[0]\n",
    "        w2, t2 = triple[1]\n",
    "        w3, t3 = triple[2]\n",
    "        cond = ((t2 == 'RB' or t2 == 'PRP' or t2.startswith('NN')) and (t3.startswith('VB') or t3 == 'BEDZ'))\n",
    "        if t1 == 'CC' and cond:\n",
    "            sents.append(sent[start:i])\n",
    "            sent = sent[i + 1:]\n",
    "            start = i + 1\n",
    "        elif i == len(bigrams) - 1:\n",
    "            sents.append(sent)\n",
    "    output = []\n",
    "    for sent in sents:\n",
    "        output.append(refine(sent))\n",
    "        \n",
    "    simplified_sents = []\n",
    "    length_so_far = 0\n",
    "    for sent in sents:\n",
    "        for simple_sent in simplify(sent, bigrams[length_so_far : length_so_far + len(sent)]):\n",
    "            simplified_sents.append(simple_sent)\n",
    "        length_so_far += len(sent)\n",
    "    \n",
    "    simplified_sents = [refine(sent) for sent in simplified_sents]\n",
    "    return simplified_sents\n",
    "\n",
    "conversion_map = defaultdict(list)\n",
    "conversion_map['adjective'] = ['a', 's']\n",
    "conversion_map['adverb'] = ['r']\n",
    "conversion_map['noun'] = ['n']\n",
    "conversion_map['verb'] = ['v']\n",
    "conversion_map['plural noun'] = ['n']\n",
    "conversion_map['transitive verb'] = ['v']\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# storing the most frequent n words, in the decreasing order of frequency\n",
    "def all_words(filename):\n",
    "    f = open(filename)\n",
    "    output = []\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        word, freq = line.split(',')\n",
    "        output.append(word)\n",
    "        i += 1\n",
    "    f.close()\n",
    "    return output\n",
    "        \n",
    "all_words = all_words('unigram_freq.csv')\n",
    "\n",
    "def freq_index(word, all_words):\n",
    "    for i in range(len(all_words)):\n",
    "        if all_words[i] == word:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "# word sense disambiguation\n",
    "def wsd(sent, word):\n",
    "    return lesk(sent, word)\n",
    "\n",
    "def define(word):\n",
    "    url = 'https://api.dictionaryapi.dev/api/v2/entries/en_US/' + word\n",
    "    try:\n",
    "        data = urlopen(url).read()\n",
    "        dictionary = json.loads(data)\n",
    "        return dictionary[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_list_of_dicts(d):\n",
    "    return isinstance(d, list) and all(isinstance(item, dict) for item in d)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    definition = define(word)\n",
    "    if not definition:\n",
    "        return None\n",
    "    meanings = definition['meanings']\n",
    "    output = defaultdict(list)\n",
    "    for meaning in meanings:\n",
    "        pos = meaning['partOfSpeech']\n",
    "        for definition in meaning['definitions']:\n",
    "            if 'synonyms' in definition:\n",
    "                output[pos].append([definition['definition'], definition['synonyms']])\n",
    "    return output\n",
    "\n",
    "def same_lemma(word1, word2):\n",
    "    try:\n",
    "        def1, def2 = define(word1), define(word2)\n",
    "        return def1['word'] == def2['word']\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def similarity_score(word_def, syn_def, syn_list):\n",
    "    for syn in syn_list:\n",
    "        index = freq_index(syn, all_words)\n",
    "        if (syn in word_def) and (index > -1) and (index < 10000):\n",
    "            return syn\n",
    "    sent1 = nltk.word_tokenize(word_def)\n",
    "    sent2 = nltk.word_tokenize(syn_def)\n",
    "    tagged_sent1 = nltk.pos_tag(sent1)\n",
    "    tagged_sent2 = nltk.pos_tag(sent2)\n",
    "    word_def = [(word.lower(), tag) for (word, tag) in tagged_sent1 if word.isalpha() and not word in stopwords]\n",
    "    syn_def = [(word.lower(), tag) for (word, tag) in tagged_sent2 if word.isalpha() and not word in stopwords]\n",
    "    score = 0\n",
    "    for (word1, tag1) in word_def:\n",
    "        for (word2, tag2) in syn_def:\n",
    "            if same_lemma(word1, word2):\n",
    "                score += 1\n",
    "                if tag1.startswith('V') and tag2.startswith('V'):\n",
    "                    score += 0.25\n",
    "    return score\n",
    "\n",
    "\n",
    "file = open('irregular_verbs.txt')\n",
    "irreg_verbs = {}\n",
    "for line in file:\n",
    "    forms = line.split()\n",
    "    verb = forms[0]\n",
    "    vbd = forms[1]\n",
    "    vbn = forms[2]\n",
    "    vbg = (forms[0] + 'ing') if not (forms[0][-1] in 'aeiou') else (forms[0][:-1] + 'ing')\n",
    "    vbz = forms[0] + 's' #(forms[0] + 'es') if not (forms[0][-1] in 'aeiou') else (forms[0] + 's')\n",
    "    irreg_verbs[verb] = {'VBD': vbd, 'VBZ': vbz, 'VBG': vbg, 'VBN': vbn, 'VBP': verb}\n",
    "    \n",
    "def pluralize(noun):\n",
    "    return engine.plural(noun)\n",
    "\n",
    "\n",
    "def comparative(adj):\n",
    "    if len(adj) > 5:\n",
    "        return adj\n",
    "    return (adj + 'r') if adj[-1] == 'e' else (adj + 'er')\n",
    "    \n",
    "    \n",
    "def superlative(adj):\n",
    "    if len(adj) > 5:\n",
    "        return adj\n",
    "    return (adj + 'st') if adj[-1] == 'e' else (adj + 'est')\n",
    "    \n",
    "            \n",
    "def handle_word(original_word, original_tag, letter, sub_word):\n",
    "    cond1 = original_tag.startswith('JJ') and letter == 's'\n",
    "    cond2 = original_tag.lower().startswith(letter)\n",
    "    if cond2:\n",
    "        if letter == 'v':\n",
    "            for end in ['ed', 'ing']:\n",
    "                if original_word.endswith(end):\n",
    "                    return ((sub_word + end) if sub_word[-1] != 'e' else (sub_word[:-1] + end))\n",
    "            else:\n",
    "                return irreg_verbs[sub_word][original_tag]\n",
    "        elif letter == 'n':\n",
    "            return sub_word\n",
    "    elif cond1:\n",
    "        if original_word.endswith('er'):\n",
    "            return comparative(sub_word)\n",
    "        elif original_word.endswith('est'):\n",
    "            return superlative(sub_word)\n",
    "        else:\n",
    "            return sub_word\n",
    "    return sub_word\n",
    "\n",
    "def easiest_synonym(tagged_sent, i, all_words):\n",
    "    word = tagged_sent[i][0]\n",
    "    basic_form = word\n",
    "    d = define(word)\n",
    "    if d:\n",
    "        basic_form = d['word']\n",
    "    fi = freq_index(basic_form, all_words)\n",
    "    if fi < 10000:\n",
    "        return word\n",
    "    elif word.isalpha() and i > 0 and i < len(tagged_sent) and word.istitle():\n",
    "        return word\n",
    "    \n",
    "    synset = wsd([w for (w, t) in tagged_sent], word)\n",
    "    if synset:\n",
    "        word_tag = synset.pos()\n",
    "        word_definition = synset.definition()\n",
    "        synonyms = get_synonyms(word) # returns a map, where keys are pos-tags\n",
    "        if synonyms:\n",
    "            easiest = None\n",
    "            index = 333333\n",
    "            for pos_tag in synonyms:\n",
    "                if word_tag in conversion_map[pos_tag] and (tagged_sent[i][1].lower().startswith(word_tag) or \n",
    "                                                            (tagged_sent[i][1].startswith('JJ') and word_tag == 's')):\n",
    "                    max_score = 0\n",
    "                    syn_list = []\n",
    "                    for n in range(len(synonyms[pos_tag])):\n",
    "                        definition, synonym_list = synonyms[pos_tag][n]\n",
    "                        score = similarity_score(word_definition, definition, synonym_list)\n",
    "                        if isinstance(score, str):\n",
    "                            return score\n",
    "                        elif score > max_score:\n",
    "                            max_score = score\n",
    "                            syn_list = synonym_list\n",
    "                    if max_score == 0 and len(synonyms[pos_tag]) == 1:\n",
    "                        for j in range(len(synonyms[pos_tag][0][1])):\n",
    "                            current_index = freq_index(synonyms[pos_tag][0][1][j], all_words)\n",
    "                            if current_index > -1 and current_index < 10000:\n",
    "                                easiest = synonyms[pos_tag][0][1][j]  \n",
    "                    else:   \n",
    "                        for synonym in syn_list:\n",
    "                            current = freq_index(synonym, all_words)\n",
    "                            if current > -1 and current < 10000:\n",
    "                                easiest = synonym\n",
    "                                break\n",
    "                    easiest = handle_word(word, tagged_sent[i][1], word_tag, easiest)\n",
    "            if easiest:\n",
    "                return easiest\n",
    "    return word\n",
    "\n",
    "def simplify_text(text, tagger, all_words):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    tokenized_sents = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    modified_sents = []\n",
    "    for sent in tokenized_sents:\n",
    "        a = decompose(sent, tagger)\n",
    "        for simple_sent in a:\n",
    "            modified_sents.append(simple_sent)\n",
    "    output = [[easiest_synonym(tagger.tag(sent), i, all_words) for i in range(len(sent))] \n",
    "            for sent in modified_sents]\n",
    "    return output\n",
    "\n",
    "def simplify_and_convert_to_text(text, tagger, all_words):\n",
    "    sents = simplify_text(text, tagger, all_words)\n",
    "    text = ''\n",
    "    for sent in sents:\n",
    "        for i in range(len(sent)):\n",
    "            if sent[i] == \"'s\":\n",
    "                text = text[:-1]\n",
    "            text += sent[i]\n",
    "            if (i < len(sent) - 1 and not (sent[i + 1] in '.,?!:;')) or i == len(sent) - 1:\n",
    "                text += ' '\n",
    "    return text\n",
    "\n",
    "class NLTK_Tagger(nltk.TaggerI): \n",
    "    def tag(self, sentence):\n",
    "        return nltk.pos_tag(sentence)\n",
    "    \n",
    "nltk_tagger = NLTK_Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animals (also called Metazoa) are multicellular eukaryotic organisms that form the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open('Animals_wiki.txt').read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animals ( also called Metazoa ) are multicellular eukaryotic bodies that form the biological kingdom Animalia. With few exceptions, animals eat organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during incomplete development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres ( 0.00033 in ) to 33.6 metres ( 110 ft ). They have complex interactions with each other and their environments, forming complex food webs. The scientific study of animals is known as zoology. \n"
     ]
    }
   ],
   "source": [
    "print(simplify_and_convert_to_text(text, nltk_tagger, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In June 1661, he was admitted to Trinity College, Cambridge, on the recommendation of his uncle Rev William Ayscough, who had studied there. He started as a subsizar—paying his way by performing valet's duties—until he was awarded a scholarship in 1664, guaranteeing him four more years until he could get his MA. At that time, the college's teachings were based on those of Aristotle, whom Newton supplemented with modern philosophers such as Descartes, and astronomers such as Galileo and Thomas Street, through whom he learned of Kepler's work. He set down in his notebook a series of \"Quaestiones\" about mechanical philosophy as he found it. In 1665, he discovered the generalised binomial theorem and began to develop a mathematical theory that later became calculus. Soon after Newton had obtained his BA degree in August 1665, the university temporarily closed as a precaution against the Great Plague. Although he had been undistinguished as a Cambridge student, Newton's private studies at his home in Woolsthorpe over the subsequent two years saw the development of his theories on calculus, optics, and the law of gravitation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open('Isaac_Newton_2.txt').read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In June 1661, he was admitted to Trinity College, Cambridge, on the recommendation of his uncle Rev William Ayscough. He/She/It/They/There had studied there. He started as a subsizar—paying his way by performing man's duties—until he was awarded a scholarship in 1664, guaranteeing him four more years until he could get his MA. At that time, the college's teachings were based on those of Aristotle and astronomers such as Galileo and Thomas Street, through whom he learned of Kepler's work. He/She/It/They/There Newton supplemented with modern philosophers such as Descartes. He set down in his notebook a series of `` Quaestiones '' about mechanical philosophy as he found it. In 1665, he discovered the generalised binomial theorem and began to develop a mathematical theory that later became calculus. Soon after Newton had obtained his BA degree in August 1665, the university temporarily closed as a insurance against the Great Plague. Although he had been undistinguished as a Cambridge student, Newton's private studies at his home in Woolsthorpe over the subsequent two years saw the development of his theories on calculus, optics, and the law of trend. \n"
     ]
    }
   ],
   "source": [
    "print(simplify_and_convert_to_text(text, nltk_tagger, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predecessor state of the Roman Empire, the Roman Republic (which had replaced Rome's monarchy in the 6th century BC) became severely destabilized in a series of civil wars and political conflicts. In the mid-1st century BC, Julius Caesar was appointed as perpetual dictator and then assassinated in 44 BC. Civil wars and proscriptions continued, culminating in the victory of Octavian, Caesar's adopted son, over Mark Antony and Cleopatra at the Battle of Actium in 31 BC. The following year Octavian conquered Ptolemaic Egypt, ending the Hellenistic period that had begun with the conquests of Alexander the Great of Macedon in the 4th century BC. Octavian's power then became unassailable, and in 27 BC the Roman Senate formally granted him overarching power and the new title Augustus, effectively making him the first Roman emperor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open('Roman_Empire.txt').read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predecessor state of the Roman Empire, the Roman Republic ( which had replaced Rome's monarchy in the 6th century BC ) became seriously upseted in a series of civil wars and political conflicts. In the mid-1st century BC, Julius Caesar was appointed as endless dictator. Then assassinated in 44 BC. Civil wars. ban continued, peaking in the victory of Octavian, Caesar's adopted son, over Mark Antony and Cleopatra at the Battle of Actium in 31 BC. The following year Octavian conquered Ptolemaic Egypt, ending the Hellenistic period that had begun with the conquests of Alexander the Great of Macedon in the 4th century BC. Octavian's power then became unassailable, and in 27 BC the Roman Senate formally granted him overarching power and the new title Augustus, effectively making him the first Roman emperor. \n"
     ]
    }
   ],
   "source": [
    "print(simplify_and_convert_to_text(text, nltk_tagger, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
